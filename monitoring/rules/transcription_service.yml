# Prometheus alerting rules for transcription service

groups:
  - name: transcription_service_alerts
    rules:
      # Application availability alerts
      - alert: ServiceDown
        expr: up{job="transcription-service"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Transcription service is down"
          description: "The transcription service has been down for more than 1 minute."

      - alert: HighErrorRate
        expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.1
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value | humanizePercentage }} for the last 5 minutes."

      # Performance alerts
      - alert: HighResponseTime
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m])) > 2
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "High response time detected"
          description: "95th percentile response time is {{ $value }}s for the last 5 minutes."

      - alert: HighMemoryUsage
        expr: container_memory_usage_bytes{name="transcription-service"} / container_spec_memory_limit_bytes{name="transcription-service"} > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High memory usage"
          description: "Memory usage is above 90% for the last 5 minutes."

      - alert: HighCPUUsage
        expr: rate(container_cpu_usage_seconds_total{name="transcription-service"}[5m]) > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High CPU usage"
          description: "CPU usage is above 80% for the last 5 minutes."

      # Database alerts
      - alert: DatabaseDown
        expr: up{job="postgres"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "PostgreSQL database is down"
          description: "PostgreSQL database has been down for more than 1 minute."

      - alert: DatabaseConnectionsHigh
        expr: pg_stat_database_numbackends / pg_settings_max_connections > 0.8
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High database connections"
          description: "Database connections are above 80% of the maximum."

      - alert: DatabaseSlowQueries
        expr: rate(pg_stat_database_tup_returned[5m]) / rate(pg_stat_database_tup_fetched[5m]) < 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Database slow queries detected"
          description: "Database query efficiency is below 10% for the last 5 minutes."

      # Redis alerts
      - alert: RedisDown
        expr: up{job="redis"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Redis is down"
          description: "Redis has been down for more than 1 minute."

      - alert: RedisMemoryUsageHigh
        expr: redis_memory_used_bytes / redis_memory_max_bytes > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Redis memory usage high"
          description: "Redis memory usage is above 90% for the last 5 minutes."

      # Celery worker alerts
      - alert: CeleryWorkerDown
        expr: up{job="celery-workers"} == 0
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Celery worker is down"
          description: "Celery worker has been down for more than 2 minutes."

      - alert: CeleryQueueBacklog
        expr: celery_queue_length > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High Celery queue backlog"
          description: "Celery queue length is {{ $value }} tasks for the last 5 minutes."

      - alert: CeleryTaskFailureRate
        expr: rate(celery_task_failed_total[5m]) / rate(celery_task_total[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High Celery task failure rate"
          description: "Celery task failure rate is {{ $value | humanizePercentage }} for the last 5 minutes."

      # Storage alerts
      - alert: DiskSpaceHigh
        expr: (node_filesystem_size_bytes - node_filesystem_free_bytes) / node_filesystem_size_bytes > 0.9
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High disk space usage"
          description: "Disk space usage is above 90% on {{ $labels.mountpoint }}."

      - alert: AudioFileProcessingStuck
        expr: increase(transcription_jobs_processing_total[1h]) == 0 and transcription_jobs_pending_total > 0
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: "Audio file processing appears stuck"
          description: "No jobs have been processed in the last hour despite pending jobs."

      # Business logic alerts
      - alert: TranscriptionJobsBacklog
        expr: transcription_jobs_pending_total > 50
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "High transcription jobs backlog"
          description: "There are {{ $value }} pending transcription jobs."

      - alert: ModelLoadingFailure
        expr: rate(ai_model_loading_failures_total[5m]) > 0
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "AI model loading failures"
          description: "AI models are failing to load at a rate of {{ $value }} per second."

      - alert: AudioConversionFailureRate
        expr: rate(audio_conversion_failures_total[5m]) / rate(audio_conversion_attempts_total[5m]) > 0.2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "High audio conversion failure rate"
          description: "Audio conversion failure rate is {{ $value | humanizePercentage }} for the last 5 minutes."

  - name: transcription_service_sla
    rules:
      # SLA monitoring
      - alert: SLAViolation
        expr: |
          (
            rate(http_requests_total{status=~"2.."}[30d]) /
            rate(http_requests_total[30d])
          ) < 0.999
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "SLA violation detected"
          description: "Service availability is below 99.9% over the last 30 days."

      - alert: ResponseTimeSLAViolation
        expr: histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[30d])) > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Response time SLA violation"
          description: "95th percentile response time is above 5 seconds over the last 30 days."